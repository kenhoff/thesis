\chapter{Procedure}
	\section{Survey Design}
		In order to test the effectiveness of the rubric on determining good educational game design elements, it must be used by real players on real educational games. To get the best results possible, it would need to be evaluated by a large number of players across a variety of games.

		Our procedure, then, would be very simple. A player would need to play a game for a short period of time, then fill out the survey, indicating what elements the game had. There's no need for additional feedback or contextual observation of the player as they play the game. This means that we can administer our survey remotely, through online quiz or survey-taking systems.

	\section{Mechanical Turk}
		I decided on Amazon's Mechanical Turk to adminster my surveys. Mechanical Turk is an online marketplace where Requesters (the people who want results from surveys or tasks) can submit HITs (Human Intelligence Tasks) into the marketplace, where Workers (people who fill out surveys or perform other tasks) can complete them, usually receiving some kind of small monetary reward for them. It's commonly used as a survey-taking platform, but Requesters also use it for tasks that computers are not yet good at doing, like determining the content of an image, writing a summary of an article, or finding the website of a lesser-known business.

% initially wanted to do all this stuff, blah blah blah

		In order to only allow a worker to take a survey once, I had to set up 10 different HITs, one for each game that we were testing. I also allocated a number of responses that I wanted to get for each HIT; this ensured that a worker would only be allowed to fill out a survey once, but was allowed to complete survey for all of the games if they wanted. I allocated 20 responses for each game, for a total of 200 survey responses. The initial expiration date of the HITs was 3 days later, but I extended the deadline a week. 

		For 6 HITs, the HIT was composed of the ``Basic" survey included below. It contained some simple demographic questions, some Likert-scale questions about their opinions on fun and educational games, and some freeform text response areas where they could leave feedback about the games and survey. It also included a link to one of 6 games (The Incredible Machine, Lemmings, Notpron, Math Baseball, BotLogic, or Pandemic 2), as well as a section with all of the rubric items, where players were asked to rate each game on all of the rubric items.

		The other 4 HITs included a pre- and post-quiz in addition to the basic survey. The 4 games (Darfur is Dying, The Oregon Trail, Number Munchers, and LightBot) each had their own quiz, designed to test the knowledge that the games were intended to teach. The quizzes were 10 questions each, with either 4 or 5 multiple choice answers, or True/False answers. It's important to note that the pre- and post-quizzes were the exact same; the exact same questions and answers appeared before and after the players played and rated the game. The players were not given feedback on how well they did on either of the quizzes. Each quiz is included below.  

	\section{Survey}
		This is the survey that was given to the Mechanical Turk workers. It includes some demographic questions, followed by some questions on the worker's opinions on fun and educational games. Then, the worker is given the URL to one of the ten games. Then, the worker is asked to rate the game on the 13 rubric items, selecting one of the 5 options from each. After that, the worker provides more opinions on the game they just played.

\input survey2

	\section{Quiz}
		These are the quizzes that were given to the Mechanical Turk workers. There are 4 quizzes, one for each game that was selected as a quiz game. The workers are given the corresponding quiz both before and after the `Survey' section; both the before and after quizzes are the exact same.

		The quizzes are each composed of 10 multiple choice questions. The questions were selected by first examining the reported educational goals of the games, then synthesizing the quizzes using existing online surveys.
\input darfur
\input oregon
\input munchers
\input lightbot

\newpage
	
% \section{Survey Design}
% Many references taken from \url{http://homepage.psy.utexas.edu/homepage/faculty/gosling/reprints/MTurkhowto.pdf}
% 	\subsection{Compensation Scheme}
% 		Mechanical Turk workers are commonly compensated at 10 cents per person per 10 minute response. This seems more than adequate; however, the rate will begin at \$0.05 per survey response. If the requisite number of responses hasn't been received in one week, the rate will increase to \$0.10 per survey response, and will continue to increase each \$0.05 each week until all the survey responses are received.
% 	\subsection{Ensuring High-Quality Responses}
% 		To ensure high-quality survey responses, there are a number of measures that can be taken. The first is the criteria option built in to Mechanical Turk for workers. It's recommended to avoid translation issues by restricting to US workers only, and to filter out workers with a bad history of responses by using a 95\% HIT acceptance rate criteria.

% 		It is harder to ensure that the workers actually play the requisite amount of the game. To combat workers not actually playing the game, the workers will be required to upload a screenshot of their final score playing the game. This screenshot can be searched for online to see if they uploaded an image found elsewhere; if so, the result will be rejected.

% 		The hardest part will be to ensure that the workers fill the form out without providing any bogus responses. There's not much that can be done for this (due to us testing to see if the rubric is excessively vague), but we can implement safegaurds such as the cleverly-hidden "answer this number for this question" section.